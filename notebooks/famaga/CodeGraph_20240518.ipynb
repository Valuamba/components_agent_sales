{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2637a3d1-3c0c-430a-8cb1-2b33c725bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Connection setup\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"16zomole\"))  # Replace with your credentials\n",
    "\n",
    "def add_ast(tx, node_id, node_type, name=None):\n",
    "    \"\"\" Function to add an AST node to the Neo4j database \"\"\"\n",
    "    if name:\n",
    "        query = (\n",
    "            \"CREATE (n:ASTNode {id: $node_id, type: $node_type, name: $name}) \"\n",
    "            \"RETURN n\"\n",
    "        )\n",
    "        result = tx.run(query, node_id=node_id, node_type=node_type, name=name)\n",
    "    else:\n",
    "        query = (\n",
    "            \"CREATE (n:ASTNode {id: $node_id, type: $node_type}) \"\n",
    "            \"RETURN n\"\n",
    "        )\n",
    "        result = tx.run(query, node_id=node_id, node_type=node_type)\n",
    "    return result.single()[0]\n",
    "\n",
    "def add_relationship(tx, parent_id, child_id):\n",
    "    \"\"\" Function to add a relationship between two AST nodes \"\"\"\n",
    "    query = (\n",
    "        \"MATCH (a:ASTNode), (b:ASTNode) \"\n",
    "        \"WHERE a.id = $parent_id AND b.id = $child_id \"\n",
    "        \"CREATE (a)-[:PARENT_OF]->(b)\"\n",
    "    )\n",
    "    tx.run(query, parent_id=parent_id, child_id=child_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e446a66-da95-4c02-950e-057b07edc9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import hashlib\n",
    "import subprocess\n",
    "import json\n",
    "from functools import wraps\n",
    "import inspect\n",
    "\n",
    "text_cache = Path('cache')\n",
    "\n",
    "def sha1(input_string):\n",
    "    \"\"\"Helper to hash input strings\"\"\"\n",
    "    try:\n",
    "\n",
    "        # Step 5: Create a new SHA-1 hash object\n",
    "        hash_object = hashlib.sha1()\n",
    "\n",
    "        # Step 6: Update the hash object with the bytes-like object\n",
    "        hash_object.update(input_string.encode('utf-8'))\n",
    "\n",
    "        # Step 7: Get the hexadecimal representation of the hash\n",
    "        return hash_object.hexdigest()\n",
    "    except Exception as e:\n",
    "        raise ValueError(input_string) from e\n",
    "\n",
    "\n",
    "def stored(func):\n",
    "    \"\"\"\n",
    "    implements nix-like durable memoisation of function results.\n",
    "\n",
    "    Lazy way to avoid recomputing expensive calls. Expects results to be JSON-serializable\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def CACHE(*args, **kwargs):\n",
    "        name = func.__name__\n",
    "        meta = {}\n",
    "\n",
    "        meta[\"name\"] = name\n",
    "        meta[\"func\"] = inspect.getsource(func)\n",
    "        meta[\"args\"] = args\n",
    "        meta[\"kwargs\"] = kwargs\n",
    "\n",
    "        js = json.dumps(meta)\n",
    "        sha = hashlib.sha1(js.encode('utf-8'))\n",
    "\n",
    "        digest = sha.hexdigest()\n",
    "\n",
    "        path = text_cache / f\"{digest}-{name}.json\"\n",
    "\n",
    "        if path.exists():\n",
    "            with path.open('r') as r:\n",
    "                cached = json.load(r)\n",
    "            return cached[\"result\"]\n",
    "        result = func(*args, **kwargs)\n",
    "        meta[\"result\"] = result\n",
    "        with path.open('w') as w:\n",
    "            json.dump(meta, w)\n",
    "        return result\n",
    "\n",
    "    return CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e26425dd-9556-47c8-8059-0057d6e51f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hardcoded AST nodes (simplified version of your structure)\n",
    "    ast_nodes = {\n",
    "        \"file\": (\"File\", \"main\"),\n",
    "        \"import\": (\"GenDecl\", \"import\"),\n",
    "        \"func\": (\"FuncDecl\", \"main\"),\n",
    "    }\n",
    "\n",
    "    # Relationships based on AST structure\n",
    "    relationships = [\n",
    "        (\"file\", \"import\"),\n",
    "        (\"file\", \"func\"),\n",
    "    ]\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # Adding nodes\n",
    "        for node_id, (node_type, name) in ast_nodes.items():\n",
    "            session.write_transaction(add_ast, node_id, node_type, name)\n",
    "\n",
    "        # Adding relationships\n",
    "        for parent_id, child_id in relationships:\n",
    "            session.write_transaction(add_relationship, parent_id, child_id)\n",
    "\n",
    "        print(\"AST loaded into Neo4j successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d857d3c4-5219-4102-9322-6d63c195fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AST loaded into Neo4j successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x2/0f74_mmn0ts5qbnsn_wp241w0000gn/T/ipykernel_5259/3396984112.py:18: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(add_ast, node_id, node_type, name)\n",
      "/var/folders/x2/0f74_mmn0ts5qbnsn_wp241w0000gn/T/ipykernel_5259/3396984112.py:22: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(add_relationship, parent_id, child_id)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f09abc47-b22d-4a4b-93ba-c5af6b02971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ast_node(tx, node_id):\n",
    "    \"\"\" Function to retrieve an AST node from the Neo4j database \"\"\"\n",
    "    query = \"MATCH (n:ASTNode {id: $node_id}) RETURN n\"\n",
    "    result = tx.run(query, node_id=node_id)\n",
    "    record = result.single()  # Get the single record if available\n",
    "    return record[\"n\"] if record else None\n",
    "\n",
    "\n",
    "def get_children(tx, parent_id):\n",
    "    \"\"\" Function to retrieve child nodes of a specified parent node \"\"\"\n",
    "    query = (\n",
    "        \"MATCH (p:ASTNode {id: $parent_id})-[:PARENT_OF]->(c:ASTNode) \"\n",
    "        \"RETURN c\"\n",
    "    )\n",
    "    result = tx.run(query, parent_id=parent_id)\n",
    "    return [record[\"c\"] for record in result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c441645-1fa7-4eb8-b302-7d51b378d401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Node: <Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:0' labels=frozenset({'ASTNode'}) properties={'name': 'main', 'id': 'file', 'type': 'File'}>\n",
      "Children of the node: [<Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:5' labels=frozenset({'ASTNode'}) properties={'name': 'main', 'id': 'func', 'type': 'FuncDecl'}>, <Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:2' labels=frozenset({'ASTNode'}) properties={'name': 'main', 'id': 'func', 'type': 'FuncDecl'}>, <Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:4' labels=frozenset({'ASTNode'}) properties={'name': 'import', 'id': 'import', 'type': 'GenDecl'}>, <Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:1' labels=frozenset({'ASTNode'}) properties={'name': 'import', 'id': 'import', 'type': 'GenDecl'}>, <Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:2' labels=frozenset({'ASTNode'}) properties={'name': 'main', 'id': 'func', 'type': 'FuncDecl'}>, <Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:1' labels=frozenset({'ASTNode'}) properties={'name': 'import', 'id': 'import', 'type': 'GenDecl'}>, <Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:5' labels=frozenset({'ASTNode'}) properties={'name': 'main', 'id': 'func', 'type': 'FuncDecl'}>, <Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:2' labels=frozenset({'ASTNode'}) properties={'name': 'main', 'id': 'func', 'type': 'FuncDecl'}>, <Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:4' labels=frozenset({'ASTNode'}) properties={'name': 'import', 'id': 'import', 'type': 'GenDecl'}>, <Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:1' labels=frozenset({'ASTNode'}) properties={'name': 'import', 'id': 'import', 'type': 'GenDecl'}>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x2/0f74_mmn0ts5qbnsn_wp241w0000gn/T/ipykernel_5259/1186447875.py:9: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  children = session.read_transaction(get_children, \"file\")\n"
     ]
    }
   ],
   "source": [
    "session = driver.session()\n",
    "node = session.execute_read(get_ast_node, \"file\")\n",
    "if node:\n",
    "    print(\"Retrieved Node:\", node)\n",
    "else:\n",
    "    print(\"Node not found.\")\n",
    "\n",
    "        # Get children of a specific node\n",
    "children = session.read_transaction(get_children, \"file\")\n",
    "print(\"Children of the node:\", [child for child in children])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebc2eeee-c666-45c9-8e00-4ad834bada1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = session.execute_read(get_ast_node, \"file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4f93ea1-8eb4-4b43-8c02-37c72f0d4f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Node element_id='4:5e1d3220-7b75-49f7-9fdd-98681d96641a:0' labels=frozenset({'ASTNode'}) properties={'name': 'main', 'id': 'file', 'type': 'File'}>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6d47e77-05ad-4f7d-b2bb-9417cb222465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Function to parse the YAML files\n",
    "def parse_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "# Extract environment and vault blocks from values.yml\n",
    "def extract_values_info(data):\n",
    "    env_block = data.get('env', None)\n",
    "    vault_block = data.get('vault', None)\n",
    "    return env_block, vault_block\n",
    "\n",
    "# Extract project name and dependencies from .gitlab-ci.yml\n",
    "def extract_gitlab_info(data):\n",
    "    project_name = data.get('variables', {}).get('CI_TMPL_HELM_RELEASE_NAMES', None)\n",
    "    registry_project = data.get('variables', {}).get('REGISTRY_PROJECT', None)\n",
    "    return project_name, registry_project\n",
    "\n",
    "# Function to create graph database\n",
    "def create_graph(env_block, vault_block, project_name, registry_project):\n",
    "    # Connect to Neo4j\n",
    "    \n",
    "    uri = \"bolt://localhost:7687\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"16zomole\"))  # Replace with your credentials\n",
    "    \n",
    "    with driver.session(database='alljobswb') as session:\n",
    "        # Create nodes and relationships in Neo4j\n",
    "        if project_name:\n",
    "            session.run(\"MERGE (p:Project {name: $name})\", name=project_name)\n",
    "        \n",
    "        if registry_project:\n",
    "            session.run(\"MERGE (r:Registry {name: $name})\", name=registry_project)\n",
    "        \n",
    "        if env_block:\n",
    "            for key, value in env_block.items():\n",
    "                session.run(\"MERGE (e:Env {key: $key, value: $value})\", key=key, value=value)\n",
    "                if project_name:\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (p:Project {name: $project_name}), (e:Env {key: $key, value: $value})\n",
    "                        MERGE (p)-[:USES_ENV]->(e)\n",
    "                    \"\"\", project_name=project_name, key=key, value=value)\n",
    "        \n",
    "        if vault_block:\n",
    "            session.run(\"MERGE (v:Vault {enabled: $enabled})\", enabled=vault_block.get('enabled', False))\n",
    "            if project_name:\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (p:Project {name: $project_name}), (v:Vault {enabled: $enabled})\n",
    "                    MERGE (p)-[:USES_VAULT]->(v)\n",
    "                \"\"\", project_name=project_name, enabled=vault_block.get('enabled', False))\n",
    "    \n",
    "    driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fab6881-87a2-4e3d-a6b4-15b8e09604a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'DEBUG': 'false',\n",
       "  'HTTP_PORT': 8080,\n",
       "  'HR_EMPLOYEES_URL': 'http://hr-employees.hr.svc.k8s.prod-dl',\n",
       "  'HR_WBUSER_URL': 'http://hr-wbusers.hr.svc.k8s.prod-dl',\n",
       "  'AUTH_URL': 'http://auth.alljobswb.svc.k8s.prod-dp',\n",
       "  'NATS_CLUSTER_URL': 'nats://hr-rabbitmq.dl.wb.ru:4222',\n",
       "  'HR_PERSONAL_ACCOUNT_URL': 'http://hr-personal-account.hr.svc.k8s.prod-dl',\n",
       "  'RBAC_URL': 'http://rbac.alljobswb.svc.k8s.prod-dl',\n",
       "  'WBJOB_URL': 'http://admin.wbjobs.svc.k8s.prod-dp',\n",
       "  'IDENTIFICATION_URL': 'http://identification.wbx-authorization.svc.k8s.auth-dl/',\n",
       "  'USER_INFO_URL': 'https://api.user-info.svc.k8s.prod-dp',\n",
       "  'PUB_KEYS_URL': 'https://auth.wb.ru/public/pubs.json'},\n",
       " None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_data = parse_yaml('/Users/valuamba/wb/requisites-api/deploy/k8s.prod-dp/values.yml')\n",
    "\n",
    "\n",
    "env_block, vault_block = extract_values_info(values_data)\n",
    "\n",
    "env_block, vault_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cddcecd0-86b8-4dcd-84fb-e5817cde26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/valuamba/wb/requisites-api/deploy/k8s.prod-dp/values.yml'\n",
    "ci_path = '/Users/valuamba/wb/requisites-api/.golangci.yml'\n",
    "\n",
    "# Parse the YAML files\n",
    "values_data = parse_yaml(path)\n",
    "gitlab_data = parse_yaml(ci_path)\n",
    "\n",
    "# Extract necessary information\n",
    "env_block, vault_block = extract_values_info(values_data)\n",
    "project_name, registry_project = extract_gitlab_info(gitlab_data)\n",
    "\n",
    "# Create graph database\n",
    "create_graph(env_block, vault_block, project_name, registry_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bdbf9cd-998a-4a9b-b386-16845e58ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "def load_yaml_with_error_handling(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(f\"Error parsing YAML file: {exc}\")\n",
    "            data = None\n",
    "    return data\n",
    "\n",
    "def load_yaml_ignore_errors(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    valid_lines = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            yaml.safe_load(line)\n",
    "            valid_lines.append(line)\n",
    "        except yaml.YAMLError:\n",
    "            continue\n",
    "\n",
    "    valid_content = ''.join(valid_lines)\n",
    "    try:\n",
    "        return yaml.safe_load(valid_content)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(f\"Error parsing combined valid YAML content: {exc}\\nPath: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract environment and vault blocks from values.yml\n",
    "def extract_values_info(data):\n",
    "    env_block = {k: v for k, v in data.get('env', {}).items() if is_url(v)}\n",
    "    vault_block = {k: v for k, v in data.get('vault', {}).items()} if data.get('vault') else None\n",
    "    return env_block, vault_block\n",
    "\n",
    "def is_url(value):\n",
    "    return isinstance(value, str) and re.match(r'http[s]?://', value) is not None\n",
    "\n",
    "def extract_project_and_namespace(url):\n",
    "    match = re.match(r'http[s]?://([^\\.]+)\\.([^\\.]+)\\.svc\\.(k8s\\..+)', url)\n",
    "    if match:\n",
    "        project_name, namespace = match.group(1), match.group(2)\n",
    "        return project_name, namespace\n",
    "    return None, None\n",
    "\n",
    "# Extract project name and dependencies from .gitlab-ci.yml\n",
    "def extract_gitlab_info(data):\n",
    "    project_name = data.get('variables', {}).get('CI_TMPL_HELM_RELEASE_NAMES', None)\n",
    "    registry_project = data.get('variables', {}).get('REGISTRY_PROJECT', None)\n",
    "    return project_name, registry_project\n",
    "\n",
    "# Function to create or merge nodes and relationships in the graph database\n",
    "def create_graph(session, project_name, env_block, vault_block, environment):\n",
    "    if project_name:\n",
    "        session.run(\"MERGE (p:Project {name: $name})\", name=project_name)\n",
    "    \n",
    "    if env_block:\n",
    "        for key, value in env_block.items():\n",
    "            ref_project, namespace = extract_project_and_namespace(value)\n",
    "            if ref_project and namespace:\n",
    "                session.run(\"MERGE (e:Env {key: $key, value: $value})\", key=key, value=value)\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (p:Project {name: $project_name}), (e:Env {key: $key, value: $value})\n",
    "                    MERGE (p)-[:USES_ENV]->(e)\n",
    "                \"\"\", project_name=project_name, key=key, value=value)\n",
    "                session.run(\"MERGE (rp:Project {name: $ref_project})\", ref_project=ref_project)\n",
    "                session.run(\"MERGE (n:Namespace {name: $namespace})\", namespace=namespace)\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (p:Project {name: $project_name}), (rp:Project {name: $ref_project})\n",
    "                    MERGE (p)-[:INTERACTS_WITH {namespace: $namespace}]->(rp)\n",
    "                \"\"\", project_name=project_name, ref_project=ref_project, namespace=namespace)\n",
    "    \n",
    "    if vault_block:\n",
    "        for secret in vault_block.get('contents', []):\n",
    "            for path, secrets in secret.items():\n",
    "                for secret_key, secret_value in secrets.items():\n",
    "                    ref_project, namespace = extract_project_and_namespace(secret_value)\n",
    "                    if ref_project and namespace:\n",
    "                        session.run(\"MERGE (v:Vault {path: $path, key: $secret_key, value: $secret_value})\", \n",
    "                                    path=path, secret_key=secret_key, secret_value=secret_value)\n",
    "                        session.run(\"\"\"\n",
    "                            MATCH (p:Project {name: $project_name}), (v:Vault {path: $path, key: $secret_key, value: $secret_value})\n",
    "                            MERGE (p)-[:USES_VAULT]->(v)\n",
    "                        \"\"\", project_name=project_name, path=path, secret_key=secret_key, secret_value=secret_value)\n",
    "                        session.run(\"MERGE (rp:Project {name: $ref_project})\", ref_project=ref_project)\n",
    "                        session.run(\"MERGE (n:Namespace {name: $namespace})\", namespace=namespace)\n",
    "                        session.run(\"\"\"\n",
    "                            MATCH (p:Project {name: $project_name}), (rp:Project {name: $ref_project})\n",
    "                            MERGE (p)-[:INTERACTS_WITH {namespace: $namespace}]->(rp)\n",
    "                        \"\"\", project_name=project_name, ref_project=ref_project, namespace=namespace)\n",
    "\n",
    "# Function to process each project directory\n",
    "def process_project(project_dir, session):\n",
    "    gitlab_ci_path = os.path.join(project_dir, '.gitlab-ci.yml')\n",
    "    deploy_dir = os.path.join(project_dir, 'deploy')\n",
    "    \n",
    "    if not os.path.isfile(gitlab_ci_path) or not os.path.isdir(deploy_dir):\n",
    "        return\n",
    "    \n",
    "    gitlab_data = load_yaml_with_error_handling(gitlab_ci_path)\n",
    "    project_name, registry_project = extract_gitlab_info(gitlab_data)\n",
    "    \n",
    "    for root, _, files in os.walk(deploy_dir):\n",
    "        for file in files:\n",
    "            try:\n",
    "                if file.endswith('values.yml'):\n",
    "                    values_path = os.path.join(root, file)\n",
    "                    values_data = load_yaml_ignore_errors(values_path)\n",
    "                    if values_data:\n",
    "                        env_block, vault_block = extract_values_info(values_data)\n",
    "                        \n",
    "                        environment = os.path.basename(os.path.dirname(values_path))\n",
    "                        create_graph(session, project_name, env_block, vault_block, environment)\n",
    "                        print(f'Processed: {file}')\n",
    "            except Exception as exc:\n",
    "                print(f'Error processing {file}: {exc}')\n",
    "\n",
    "# Main function to iterate over all projects and create the graph\n",
    "def main(base_dir):\n",
    "    uri = \"bolt://localhost:7687\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"16zomole\"))  # Use your credentials\n",
    "    \n",
    "    with driver.session(database=\"alljobswb\") as session:\n",
    "        for project in os.listdir(base_dir):\n",
    "            project_dir = os.path.join(base_dir, project)\n",
    "            if os.path.isdir(project_dir):\n",
    "                process_project(project_dir, session)\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "# Replace 'path/to/projects' with the actual path to the directory containing your projects\n",
    "# main('path/to/projects')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba508e-6e6e-431a-b663-b6abc1d1ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dicts = [\n",
    "    '/Users/valuamba/wb/alljobswb/alljobswb',\n",
    "    '/Users/valuamba/wb/hr',\n",
    "    '/Users/valuamba/wb/hr/hr'\n",
    "]\n",
    "\n",
    "for path in group_dicts[1:]:\n",
    "    main(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0562bef-9576-4832-8f57-1601c73a364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/valuamba/wb/executors-api/deploy/common/base-values.yml'\n",
    "with open(file_path, 'r') as file:\n",
    "    yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1c688c6-eb58-4bb0-a622-bb474648de72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed YAML data:\n",
      "{'replicaCount': 1, 'image': {'pullPolicy': 'IfNotPresent'}, 'ports': [{'name': 'http', 'containerPort': 8080, 'protocol': 'TCP'}], 'annotations': {'prometheus.io/scrape': 'true', 'prometheus.io/path': '/metrics', 'prometheus.io/port': '8080'}, 'readinessProbe': {'tcpSocket': {'port': 8080}, 'initialDelaySeconds': 5, 'periodSeconds': 10}, 'livenessProbe': {'tcpSocket': {'port': 8080}, 'initialDelaySeconds': 15, 'periodSeconds': 20}, 'imagePullSecrets': [{'name': 'gitlab-registry-secret'}, {'name': 'harbor-registry-secret'}], 'resources': {'requests': {'memory': '100Mi', 'cpu': '50m'}, 'limits': {'memory': '200Mi', 'cpu': '100m'}}, 'service': {'type': 'NodePort', 'externalTrafficPolicy': 'Local', 'ports': [{'name': 'http', 'port': 80, 'protocol': 'TCP', 'targetPort': 8080}, {'protocol': 'TCP', 'name': 'https', 'port': 443, 'targetPort': 443}]}, 'env': {'DEBUG': 'true', 'DEFAULT_AGREEMENT_ID': 1, 'DEFAULT_EXECUTOR_STATUS_ID': 1, 'ADMINKA_API_URL': 'http://admin-api.alljobswb'}, 'vault': {'enabled': True, 'env': [{'services/alljobswb/{{.Helm.Release.Environment}}/secrets/{{.Helm.Release.Cluster}}/executors-api': {'FNS_SMNPD_TOKEN': 'smnpd_token', 'WBBALANCE_KEY': 'wbbalance_key', 'USER_INFO_CLIENT': 'user_info_client_id', 'USER_INFO_TOKEN': 'user_info_client_token', 'ADMINKA_API_TOKEN': 'adminka_key', 'BILLING_TOKEN': 'billing_token'}}, {'services/alljobswb/{{.Helm.Release.Environment}}/postgres/user': {'PG_USER': 'login', 'PG_PASSWORD': 'password'}}, {'services/alljobswb/{{.Helm.Release.Environment}}/astra': {'ASTRA_EDI_DOC_URL': 'edi_doc_url', 'ASTRA_EDI_LOGIN': 'edi_login', 'ASTRA_EDI_PASSWORD': 'edi_pass', 'HR_EMPL_SUPPLIER_URL': 'hr_supplier_url', 'HR_USER_SUPPLIER_URL': 'hr_user_supplier_url', 'HR_SUPPLIER_TOKEN': 'hr_supplier_token'}}, {'services/alljobswb/{{.Helm.Release.Environment}}/nats': {'NATS_URL': 'alljobswb_nats'}}, {'services/alljobswb/{{.Helm.Release.Environment}}/secrets/{{.Helm.Release.Cluster}}/shared': {'WBBALANCE_URL': 'user_balance_user_balance_url'}}]}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "def load_yaml_with_error_handling(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(f\"Error parsing YAML file: {exc}\")\n",
    "            data = None\n",
    "    return data\n",
    "\n",
    "def load_yaml_ignore_errors(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    valid_lines = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            yaml.safe_load(line)\n",
    "            valid_lines.append(line)\n",
    "        except yaml.YAMLError:\n",
    "            continue\n",
    "\n",
    "    valid_content = ''.join(valid_lines)\n",
    "    try:\n",
    "        return yaml.safe_load(valid_content)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(f\"Error parsing combined valid YAML content: {exc}\")\n",
    "        return None\n",
    "\n",
    "data = load_yaml_ignore_errors(file_path)\n",
    "if data:\n",
    "    print(\"Parsed YAML data:\")\n",
    "    print(data)\n",
    "else:\n",
    "    print(\"Failed to parse YAML data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e57a2035-9b6f-420f-b1f6-0b317d3ff3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/valuamba/wb/executors-api/deploy/common/base-values.yml'\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "def load_yaml_with_error_handling(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(f\"Error parsing YAML file: {exc}\")\n",
    "            data = None\n",
    "    return data\n",
    "\n",
    "def load_yaml_ignore_errors(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    valid_lines = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            yaml.safe_load(line)\n",
    "            valid_lines.append(line)\n",
    "        except yaml.YAMLError:\n",
    "            continue\n",
    "\n",
    "    valid_content = ''.join(valid_lines)\n",
    "    try:\n",
    "        return yaml.safe_load(valid_content)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(f\"Error parsing combined valid YAML content: {exc}\\nPath: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract environment and vault blocks from values.yml\n",
    "def extract_values_info(data):\n",
    "    env_block = {k: v for k, v in data.get('env', {}).items() if is_url(v)}\n",
    "    vault_block = {k: v for k, v in data.get('vault', {}).items()} if data.get('vault') else None\n",
    "    return env_block, vault_block\n",
    "\n",
    "def is_url(value):\n",
    "    return re.match(r'http[s]?://', value) is not None\n",
    "\n",
    "def extract_project_and_namespace(url):\n",
    "    match = re.match(r'http[s]?://([^\\.]+)\\.([^\\.]+)\\.svc\\.(k8s\\..+)', url)\n",
    "    if match:\n",
    "        project_name, namespace = match.group(1), match.group(2)\n",
    "        return project_name, namespace\n",
    "    return None, None\n",
    "\n",
    "# Extract project name and dependencies from .gitlab-ci.yml\n",
    "def extract_gitlab_info(data):\n",
    "    project_name = data.get('variables', {}).get('CI_TMPL_HELM_RELEASE_NAMES', None)\n",
    "    registry_project = data.get('variables', {}).get('REGISTRY_PROJECT', None)\n",
    "    return project_name, registry_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b025e0f-65d2-4571-aeb1-bd7ddb49fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_yaml_ignore_errors(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2f6fb-3dac-4418-bed9-f9d7e111acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_block, vault_block = extract_values_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "880e1eaf-b2a0-4451-a95a-748ac9efe6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import hvac\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "def preprocess_yaml(file_path, environment, cluster):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        # Ignore commented lines\n",
    "        if line.strip().startswith('#'):\n",
    "            continue\n",
    "        # Ignore lines with problematic variables\n",
    "        if any(var in line for var in ['{{ .Helm.Release.Namespace }}', '{{.Helm.Release.Image}}', '{{.Helm.Release.Tag}}']):\n",
    "            continue\n",
    "        # Replace environment and cluster placeholders\n",
    "        line = line.replace('{{.Helm.Release.Environment}}', environment).replace('{{.Helm.Release.Cluster}}', cluster)\n",
    "        filtered_lines.append(line)\n",
    "    \n",
    "    return ''.join(filtered_lines)\n",
    "\n",
    "def load_yaml(file_path, environment, cluster):\n",
    "    try:\n",
    "        preprocessed_content = preprocess_yaml(file_path, environment, cluster)\n",
    "        return yaml.safe_load(preprocessed_content)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(f\"Error parsing YAML file at {file_path}: {exc}\")\n",
    "        return None\n",
    "\n",
    "def load_yaml_ignore_errors(file_path, environment, cluster):\n",
    "    try:\n",
    "        preprocessed_content = preprocess_yaml(file_path, environment, cluster)\n",
    "        return yaml.safe_load(preprocessed_content)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(f\"Error parsing combined valid YAML content at {file_path}: {exc}\")\n",
    "        return None\n",
    "\n",
    "def extract_values_info(data):\n",
    "    env_block = data.get('env', {})\n",
    "    vault_block = data.get('vault', None)\n",
    "    return env_block, vault_block\n",
    "\n",
    "def extract_project_and_namespace(url):\n",
    "    match = re.match(r'http[s]?://([^\\.]+)\\.([^\\.]+)\\.svc\\.(k8s\\..+)', url)\n",
    "    if match:\n",
    "        project_name, namespace = match.group(1), match.group(2)\n",
    "        return project_name, namespace\n",
    "    return None, None\n",
    "\n",
    "def extract_gitlab_info(data):\n",
    "    project_name = data.get('variables', {}).get('CI_TMPL_HELM_RELEASE_NAMES', None)\n",
    "    registry_project = data.get('variables', {}).get('REGISTRY_PROJECT', None)\n",
    "    return project_name, registry_project\n",
    "\n",
    "def determine_environment(folder_name):\n",
    "    if \"prod\" in folder_name:\n",
    "        return \"prd\"\n",
    "    elif \"stage\" in folder_name:\n",
    "        return \"stage\"\n",
    "    elif \"dev\" in folder_name:\n",
    "        return \"dev\"\n",
    "    return None\n",
    "\n",
    "def fetch_vault_values(vault_block, environment, cluster, vault_url, vault_token):\n",
    "    # client = hvac.Client(url=vault_url, token=vault_token)\n",
    "    secrets = {}\n",
    "    if vault_block and vault_block.get('env'):\n",
    "        for secret in vault_block.get('env', []):\n",
    "            for path_template, secret_keys in secret.items():\n",
    "                path = path_template.replace('{{.Helm.Release.Environment}}', environment).replace('{{.Helm.Release.Cluster}}', cluster)\n",
    "                # response = client.read(path)\n",
    "                print(path)\n",
    "                if response and 'data' in response:\n",
    "                    for secret_key in secret_keys:\n",
    "                        secrets[secret_keys[secret_key]] = response['data'].get(secret_keys[secret_key])\n",
    "    return secrets\n",
    "\n",
    "def create_graph(session, project_name, env_block, vault_values, environment):\n",
    "    if project_name:\n",
    "        session.run(\"MERGE (p:Project {name: $name})\", name=project_name)\n",
    "    \n",
    "    if env_block:\n",
    "        for key, value in env_block.items():\n",
    "            if isinstance(value, str) and re.match(r'http[s]?://', value):\n",
    "                ref_project, namespace = extract_project_and_namespace(value)\n",
    "                if ref_project and namespace:\n",
    "                    session.run(\"MERGE (e:Env {key: $key, value: $value})\", key=key, value=value)\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (p:Project {name: $project_name}), (e:Env {key: $key, value: $value})\n",
    "                        MERGE (p)-[:USES_ENV]->(e)\n",
    "                    \"\"\", project_name=project_name, key=key, value=value)\n",
    "                    session.run(\"MERGE (rp:Project {name: $ref_project})\", ref_project=ref_project)\n",
    "                    session.run(\"MERGE (n:Namespace {name: $namespace})\", namespace=namespace)\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (p:Project {name: $project_name}), (rp:Project {name: $ref_project})\n",
    "                        MERGE (p)-[:INTERACTS_WITH {namespace: $namespace}]->(rp)\n",
    "                    \"\"\", project_name=project_name, ref_project=ref_project, namespace=namespace)\n",
    "    \n",
    "    if vault_values:\n",
    "        for secret_key, secret_value in vault_values.items():\n",
    "            if isinstance(secret_value, str) and re.match(r'http[s]?://', secret_value):\n",
    "                ref_project, namespace = extract_project_and_namespace(secret_value)\n",
    "                if ref_project and namespace:\n",
    "                    session.run(\"MERGE (v:Vault {key: $secret_key, value: $secret_value})\", \n",
    "                                secret_key=secret_key, secret_value=secret_value)\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (p:Project {name: $project_name}), (v:Vault {key: $secret_key, value: $secret_value})\n",
    "                        MERGE (p)-[:USES_VAULT]->(v)\n",
    "                    \"\"\", project_name=project_name, secret_key=secret_key, secret_value=secret_value)\n",
    "                    session.run(\"MERGE (rp:Project {name: $ref_project})\", ref_project=ref_project)\n",
    "                    session.run(\"MERGE (n:Namespace {name: $namespace})\", namespace=namespace)\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (p:Project {name: $project_name}), (rp:Project {name: $ref_project})\n",
    "                        MERGE (p)-[:INTERACTS_WITH {namespace: $namespace}]->(rp)\n",
    "                    \"\"\", project_name=project_name, ref_project=ref_project, namespace=namespace)\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    if dict1 is None:\n",
    "        return dict2\n",
    "    if dict2 is None:\n",
    "        return dict1\n",
    "    result = dict1.copy()\n",
    "    for key, value in dict2.items():\n",
    "        if isinstance(value, dict) and key in result:\n",
    "            result[key] = merge_dicts(result[key], value)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    return result\n",
    "\n",
    "def process_project(project_dir, session, vault_url, vault_token):\n",
    "    gitlab_ci_path = os.path.join(project_dir, '.gitlab-ci.yml')\n",
    "    deploy_dir = os.path.join(project_dir, 'deploy')\n",
    "    common_values_path = os.path.join(deploy_dir, 'common', 'base-values.yml')\n",
    "    \n",
    "    if not os.path.isfile(gitlab_ci_path) or not os.path.isdir(deploy_dir):\n",
    "        return\n",
    "    \n",
    "    environment = determine_environment(deploy_dir)\n",
    "    cluster = os.path.basename(deploy_dir)\n",
    "\n",
    "    gitlab_data = load_yaml(gitlab_ci_path, environment, cluster)\n",
    "    if gitlab_data is None:\n",
    "        print(f\"Skipping project {project_dir} due to invalid .gitlab-ci.yml\")\n",
    "        return\n",
    "\n",
    "    project_name, registry_project = extract_gitlab_info(gitlab_data)\n",
    "    \n",
    "    common_values_data = load_yaml(common_values_path, environment, cluster) if os.path.isfile(common_values_path) else {}\n",
    "\n",
    "    if common_values_data is None:\n",
    "        print(f\"Error processing {common_values_path}: invalid YAML content\")\n",
    "    \n",
    "    for root, _, files in os.walk(deploy_dir):\n",
    "        for file in files:\n",
    "            try:\n",
    "                if file.endswith('values.yml'):\n",
    "                    values_path = os.path.join(root, file)\n",
    "                    values_data = load_yaml_ignore_errors(values_path, environment, cluster)\n",
    "                    if values_data:\n",
    "                        merged_values_data = merge_dicts(common_values_data, values_data)\n",
    "                        env_block, vault_block = extract_values_info(merged_values_data)\n",
    "                        \n",
    "                        vault_values = fetch_vault_values(vault_block, environment, cluster, vault_url, vault_token)\n",
    "                        create_graph(session, project_name, env_block, vault_values, environment)\n",
    "                        print(f'Processed: {file}')\n",
    "            except Exception as exc:\n",
    "                print(f'Error processing {file}: {exc}')\n",
    "\n",
    "def main(base_dir, vault_url, vault_token):\n",
    "    uri = \"bolt://localhost:7687\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(\"alljobswb\", \"16zomole\"))  # Use your credentials\n",
    "    \n",
    "    with driver.session(database=\"alljobswb\") as session:\n",
    "        for project in os.listdir(base_dir):\n",
    "            project_dir = os.path.join(base_dir, project)\n",
    "            if os.path.isdir(project_dir):\n",
    "                process_project(project_dir, session, vault_url, vault_token)\n",
    "    \n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509bf4c-1589-4dac-883a-039989be2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "vault_token = 'hvs.CAESIMz_vD1qEobGAkG0lON-YmKsqgQp9y07RHmc3CY2JcKPGh4KHGh2cy5oN0ljT3FQS3NOMThYNEJRUm5acDNQYkk'\n",
    "vault_url = 'https://vault.wildberries.ru:8200'\n",
    "\n",
    "main('/Users/valuamba/wb/alljobswb/alljobswb/', vault_url, vault_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da4c68-3788-4482-bbdf-14526e0b3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/valuamba/wb/alljobswb/alljobswb/customfields-api/deploy/k8s.stage-xc/values.yml'\n",
    "file_path = '/Users/valuamba/wb/alljobswb/alljobswb/certificates-api/deploy/common/base-values.yml'\n",
    "\n",
    "\n",
    "def preprocess_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        # Ignore commented lines\n",
    "        if line.strip().startswith('#'):\n",
    "            continue\n",
    "        # Ignore lines with problematic variables\n",
    "        if any(var in line for var in ['{{ .Helm.Release.Namespace }}', '{{.Helm.Release.Image}}', '{{.Helm.Release.Tag}}', '{{.Helm.Release.Environment}}', '{{.Helm.Release.Cluster}}']):\n",
    "            continue\n",
    "        filtered_lines.append(line)\n",
    "    \n",
    "    return ''.join(filtered_lines)\n",
    "\n",
    "def load_yaml(file_path):\n",
    "    try:\n",
    "        preprocessed_content = preprocess_yaml(file_path)\n",
    "        return yaml.safe_load(preprocessed_content)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(f\"Error parsing YAML file at {file_path}: {exc}\")\n",
    "        return None\n",
    "\n",
    "print(preprocess_yaml(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "742efbe6-f416-41bf-a5d2-8bcf0bdd927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/valuamba/wb/executors-api/deploy/common/base-values.yml'\n",
    "file_path = '/Users/valuamba/wb/alljobswb/alljobswb/certificates-api/deploy/common/base-values.yml'\n",
    "\n",
    "\n",
    "data = load_yaml_ignore_errors(file_path)\n",
    "env, vault = extract_values_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a8dacddd-2789-45c2-a2d1-7d0ac53886a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: True\n"
     ]
    }
   ],
   "source": [
    "token = 'hvs.CAESIMz_vD1qEobGAkG0lON-YmKsqgQp9y07RHmc3CY2JcKPGh4KHGh2cy5oN0ljT3FQS3NOMThYNEJRUm5acDNQYkk'\n",
    "client = hvac.Client(url='https://vault.wildberries.ru:8200', token=token)\n",
    "\n",
    "\n",
    "res = client.is_authenticated()\n",
    "print(\"res:\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9763c7df-3b8f-413c-bc88-a80b550ad1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_vault_values(vault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687080f-141a-4da4-b41b-9b44167f1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://vault.wildberries.ru:8200/ui/vault/secrets/services/list/alljobswb/stage/secrets/k8s.stage-xc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3899d257-727c-4a41-b2a1-20d0d9534755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enabled': True,\n",
       " 'env': [{'services/alljobswb/{{.Helm.Release.Environment}}/secrets/{{.Helm.Release.Cluster}}/executors-api': {'FNS_SMNPD_TOKEN': 'smnpd_token',\n",
       "    'WBBALANCE_KEY': 'wbbalance_key',\n",
       "    'USER_INFO_CLIENT': 'user_info_client_id',\n",
       "    'USER_INFO_TOKEN': 'user_info_client_token',\n",
       "    'ADMINKA_API_TOKEN': 'adminka_key',\n",
       "    'BILLING_TOKEN': 'billing_token'}},\n",
       "  {'services/alljobswb/{{.Helm.Release.Environment}}/postgres/user': {'PG_USER': 'login',\n",
       "    'PG_PASSWORD': 'password'}},\n",
       "  {'services/alljobswb/{{.Helm.Release.Environment}}/astra': {'ASTRA_EDI_DOC_URL': 'edi_doc_url',\n",
       "    'ASTRA_EDI_LOGIN': 'edi_login',\n",
       "    'ASTRA_EDI_PASSWORD': 'edi_pass',\n",
       "    'HR_EMPL_SUPPLIER_URL': 'hr_supplier_url',\n",
       "    'HR_USER_SUPPLIER_URL': 'hr_user_supplier_url',\n",
       "    'HR_SUPPLIER_TOKEN': 'hr_supplier_token'}},\n",
       "  {'services/alljobswb/{{.Helm.Release.Environment}}/nats': {'NATS_URL': 'alljobswb_nats'}},\n",
       "  {'services/alljobswb/{{.Helm.Release.Environment}}/secrets/{{.Helm.Release.Cluster}}/shared': {'WBBALANCE_URL': 'user_balance_user_balance_url'}}]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2feb2-276f-4714-9ef0-052ba2369b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "services/alljobswb/{{.Helm.Release.Environment}}/secrets/{{.Helm.Release.Cluster}}/executors-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31eb5c86-7a2a-4017-a7f6-fcaaa05ac915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace9c19-caf9-46c6-8a11-1a9c43d2633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@stored\n",
    "def get_client(path):\n",
    "    return client.read(path)\n",
    "\n",
    "\n",
    "get_client('services/alljobswb/stage/secrets/k8s.stage-xc/crm-api')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd069a1f-ea12-4019-975f-dbc6d634b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "services/list/alljobswb/stage/secrets/k8s.stage-xc/executors-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47ae5bc3-7c25-48ae-955a-1efc4fde7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import re\n",
    "import hvac\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "neo4j_uri = \"bolt://localhost:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"16zomole\"\n",
    "neo4j_db = 'alljobswbv2'\n",
    "\n",
    "# Define Vault connection details\n",
    "vault_url = \"https://vault.wildberries.ru:8200\"\n",
    "vault_token = \"hvs.CAESIM6thdfIasMeAKS-xQF2TtbZhG1vXAewgAzFGOxnllVnGh4KHGh2cy5qWlVvQ0dmb0N0MVVtazh5TGtmZUVOOXk\"\n",
    "\n",
    "# Initialize Vault client\n",
    "vault_client = hvac.Client(url=vault_url, token=vault_token)\n",
    "\n",
    "\n",
    "def determine_environment(folder_name):\n",
    "    if \"prod\" in folder_name:\n",
    "        return \"prd\"\n",
    "    elif \"stage\" in folder_name:\n",
    "        return \"stage\"\n",
    "    elif \"dev\" in folder_name:\n",
    "        return \"dev\"\n",
    "    return None\n",
    "\n",
    "def preprocess_yaml(file_path, environment, cluster):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        # Ignore commented lines\n",
    "        if line.strip().startswith('#'):\n",
    "            continue\n",
    "        # Replace environment and cluster variables\n",
    "        if re.search(r'\\{\\{\\s*\\.Helm\\.Release\\.Environment\\s*\\}\\}', line):\n",
    "            line = re.sub(r'\\{\\{\\s*\\.Helm\\.Release\\.Environment\\s*\\}\\}', environment, line)\n",
    "        if re.search(r'\\{\\{\\s*\\.Helm\\.Release\\.Cluster\\s*\\}\\}', line):\n",
    "            line = re.sub(r'\\{\\{\\s*\\.Helm\\.Release\\.Cluster\\s*\\}\\}', cluster, line)\n",
    "        # Ignore other variables\n",
    "        pattern = re.compile(r'\\{\\{\\s*\\.Helm\\.Release\\.(Namespace|Image|Tag)\\s*\\}\\}')\n",
    "        if pattern.search(line):\n",
    "            continue\n",
    "        filtered_lines.append(line)\n",
    "    \n",
    "    return ''.join(filtered_lines)\n",
    "\n",
    "def parse_yaml_file(filepath, environment, cluster):\n",
    "    preprocessed_content = preprocess_yaml(filepath, environment, cluster)\n",
    "    return yaml.safe_load(preprocessed_content)\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    for key, value in dict2.items():\n",
    "        if key in dict1:\n",
    "            if isinstance(dict1[key], dict) and isinstance(value, dict):\n",
    "                merge_dicts(dict1[key], value)\n",
    "            else:\n",
    "                dict1[key] = value\n",
    "        else:\n",
    "            dict1[key] = value\n",
    "\n",
    "def extract_values_info(data):\n",
    "    env_block = data.get('env', {})\n",
    "    vault_block = data.get('vault', None)\n",
    "    return env_block, vault_block\n",
    "\n",
    "def extract_values(folder_path):\n",
    "    env_cluster_values = {}\n",
    "    common_base_values_path = os.path.join(folder_path, 'common', 'base-values.yml')\n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file == 'values.yml':\n",
    "                environment = determine_environment(subdir)\n",
    "                cluster = os.path.basename(subdir)\n",
    "                cluster_values_path = os.path.join(subdir, file)\n",
    "\n",
    "                # Parse common values if the file exists\n",
    "                common_values = parse_yaml_file(common_base_values_path, environment, cluster) if os.path.exists(common_base_values_path) else {}\n",
    "                # Parse cluster-specific values\n",
    "                cluster_values = parse_yaml_file(cluster_values_path, environment, cluster)\n",
    "                \n",
    "                # Extract env and vault blocks from both files\n",
    "                common_env_block, common_vault_block = extract_values_info(common_values)\n",
    "                cluster_env_block, cluster_vault_block = extract_values_info(cluster_values)\n",
    "                \n",
    "                # Merge env and vault blocks, with cluster-specific values taking precedence\n",
    "                merged_env_block = common_env_block.copy()\n",
    "                merge_dicts(merged_env_block, cluster_env_block)\n",
    "                \n",
    "                merged_vault_block = common_vault_block if common_vault_block else {}\n",
    "                if cluster_vault_block:\n",
    "                    merge_dicts(merged_vault_block, cluster_vault_block)\n",
    "                \n",
    "                env_cluster_values[f'{environment}_{cluster}'] = {\n",
    "                    'vault': merged_vault_block,\n",
    "                    'env': merged_env_block\n",
    "                }\n",
    "\n",
    "    return env_cluster_values\n",
    "\n",
    "@stored\n",
    "def get_vault_value(path: str):\n",
    "    return vault_client.read(path=path)\n",
    "\n",
    "\n",
    "def fetch_vault_secrets(vault_data):\n",
    "    secrets = {}\n",
    "    if vault_data and vault_data['enabled']:\n",
    "        for vault_entry in vault_data['env']:\n",
    "            for path, keys in vault_entry.items():\n",
    "                vault_response = get_vault_value(path=path)\n",
    "                # print(vault_response)\n",
    "                vault_secrets = vault_response['data']\n",
    "                for key, alias in keys.items():\n",
    "                    if alias in vault_secrets:\n",
    "                        secrets[key] = vault_secrets[alias]\n",
    "    return secrets\n",
    "\n",
    "def extract_project_and_namespace(url):\n",
    "    # Regex to capture from start of the domain to the second dot\n",
    "    match = re.match(r'https?://([^\\.]+)\\.([^\\.]+)', url)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2)\n",
    "    return None, None\n",
    "\n",
    "def create_neo4j_graph(configs):\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "    print(neo4j_db)\n",
    "    with driver.session(database=neo4j_db) as session:\n",
    "        for project, values in configs.items():\n",
    "            environment, cluster = project.split('_')\n",
    "            vault = values['vault']\n",
    "            vault_secrets = fetch_vault_secrets(vault)\n",
    "            if vault and vault['enabled']:\n",
    "                for secret_key, secret_value in vault_secrets.items():\n",
    "                            if isinstance(secret_value, str) and re.match(r'http[s]?://', secret_value):\n",
    "                                # print('pp')\n",
    "                                ref_project, namespace = extract_project_and_namespace(secret_value)\n",
    "                                print(secret_value)\n",
    "                                if ref_project and namespace:\n",
    "                                    session.run(\"MERGE (v:Vault {key: $secret_key, value: $secret_value})\", \n",
    "                                                secret_key=secret_key, secret_value=secret_value)\n",
    "                                    session.run(\"\"\"\n",
    "                                        MATCH (p:Project {name: $project_name}), (v:Vault {key: $secret_key, value: $secret_value})\n",
    "                                        MERGE (p)-[:USES_VAULT]->(v)\n",
    "                                    \"\"\", project_name=project, secret_key=secret_key, secret_value=secret_value)\n",
    "                                    session.run(\"MERGE (rp:Project {name: $ref_project})\", ref_project=ref_project)\n",
    "                                    session.run(\"MERGE (n:Namespace {name: $namespace})\", namespace=namespace)\n",
    "                                    session.run(\"\"\"\n",
    "                                        MATCH (p:Project {name: $project_name}), (rp:Project {name: $ref_project})\n",
    "                                        MERGE (p)-[:INTERACTS_WITH {namespace: $namespace}]->(rp)\n",
    "                                    \"\"\", project_name=project, ref_project=ref_project, namespace=namespace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adbee7ce-0cab-49ad-8809-a19d22a68dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alljobswbv2\n",
      "https://edi-wb-documents.finance.svc.k8s.prod-dl\n",
      "http://hr-employees.hr.svc.k8s.prod-dl\n",
      "http://hr-wbusers.hr.svc.k8s.prod-dl\n",
      "http://user-balance.user-balance-el.wb.ru\n",
      "https://edi-wb-documents.finance.svc.k8s.prod-dl\n",
      "http://hr-employees.hr.svc.k8s.prod-dl\n",
      "http://hr-wbusers.hr.svc.k8s.prod-dl\n",
      "http://user-balance.user-balance-el.wb.ru\n",
      "http://edi-wb-documents.finance.svc.k8s.dataline1\n",
      "http://user-balance.user-balance.svc.k8s.stage-dp\n",
      "https://edi-wb-documents.finance.svc.k8s.dataline\n",
      "http://hr-employees.hr.svc.k8s.prod-dl\n",
      "http://hr-wbusers.hr.svc.k8s.prod-dl\n",
      "http://user-balance.user-balance-dev.wb.ru\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/Users/valuamba/wb/alljobswb/alljobswb/executors-api/deploy'\n",
    "values = extract_values(folder_path)\n",
    "create_neo4j_graph(values)\n",
    "# for key, value in values.items():\n",
    "#     print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87b505-e31b-434f-a59c-034c0397e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'deploy'\n",
    "values = extract_values('/Users/valuamba/wb/alljobswb/alljobswb/executors-api/deploy')\n",
    "# for key, value in values.items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49320e-98aa-4d6e-9afc-85bdf248aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda17442-2763-419c-b895-41f25d002531",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_yaml_file('/Users/valuamba/wb/alljobswb/alljobswb/certificates-api/deploy/common/base-values.yml', 'prd', 'k8s.stage-xc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a8a03-f7b8-49d7-a578-f041001e6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15d041-2e05-4799-944a-458d1b3549ed",
   "metadata": {},
   "source": [
    "## V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10f73ad0-7fed-421f-8a89-8120d51c5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import re\n",
    "import hvac\n",
    "from neo4j import GraphDatabase\n",
    "import traceback\n",
    "\n",
    "neo4j_uri = \"bolt://localhost:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"16zomole\"\n",
    "neo4j_db = 'alljobswbv2'\n",
    "\n",
    "# Define Vault connection details\n",
    "vault_url = \"https://vault.wildberries.ru:8200\"\n",
    "vault_token = \"hvs.CAESIM6thdfIasMeAKS-xQF2TtbZhG1vXAewgAzFGOxnllVnGh4KHGh2cy5qWlVvQ0dmb0N0MVVtazh5TGtmZUVOOXk\"\n",
    "\n",
    "# Initialize Vault client\n",
    "vault_client = hvac.Client(url=vault_url, token=vault_token)\n",
    "\n",
    "def determine_environment(folder_name):\n",
    "    if \"prod\" in folder_name:\n",
    "        return \"prd\"\n",
    "    elif \"stage\" in folder_name:\n",
    "        return \"stage\"\n",
    "    elif \"dev\" in folder_name:\n",
    "        return \"dev\"\n",
    "    return None\n",
    "\n",
    "def preprocess_yaml(file_path, environment, cluster):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        # Ignore commented lines\n",
    "        if line.strip().startswith('#'):\n",
    "            continue\n",
    "        # Replace environment and cluster variables\n",
    "        if re.search(r'\\{\\{\\s*\\.Helm\\.Release\\.Environment\\s*\\}\\}', line):\n",
    "            line = re.sub(r'\\{\\{\\s*\\.Helm\\.Release\\.Environment\\s*\\}\\}', environment, line)\n",
    "        if re.search(r'\\{\\{\\s*\\.Helm\\.Release\\.Cluster\\s*\\}\\}', line):\n",
    "            line = re.sub(r'\\{\\{\\s*\\.Helm\\.Release\\.Cluster\\s*\\}\\}', cluster, line)\n",
    "        # Ignore other variables\n",
    "        pattern = re.compile(r'\\{\\{\\s*\\.Helm\\.Release\\.(Namespace|Image|Tag)\\s*\\}\\}')\n",
    "        if pattern.search(line):\n",
    "            continue\n",
    "        filtered_lines.append(line)\n",
    "    \n",
    "    return ''.join(filtered_lines)\n",
    "\n",
    "def parse_yaml_file(filepath, environment, cluster):\n",
    "    preprocessed_content = preprocess_yaml(filepath, environment, cluster)\n",
    "    return yaml.safe_load(preprocessed_content)\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    for key, value in dict2.items():\n",
    "        if key in dict1:\n",
    "            if isinstance(dict1[key], dict) and isinstance(value, dict):\n",
    "                merge_dicts(dict1[key], value)\n",
    "            else:\n",
    "                dict1[key] = value\n",
    "        else:\n",
    "            dict1[key] = value\n",
    "\n",
    "def extract_values_info(data):\n",
    "    env_block = data.get('env', {})\n",
    "    vault_block = data.get('vault', None)\n",
    "    return env_block, vault_block\n",
    "\n",
    "def extract_values(folder_path):\n",
    "    env_cluster_values = {}\n",
    "    common_base_values_path = os.path.join(folder_path, 'common', 'base-values.yml')\n",
    "\n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file == 'values.yml':\n",
    "                environment = determine_environment(subdir)\n",
    "                cluster = os.path.basename(subdir)\n",
    "                cluster_values_path = os.path.join(subdir, file)\n",
    "\n",
    "                # Parse common values if the file exists\n",
    "                common_values = parse_yaml_file(common_base_values_path, environment, cluster) if os.path.exists(common_base_values_path) else {}\n",
    "                # Parse cluster-specific values\n",
    "                cluster_values = parse_yaml_file(cluster_values_path, environment, cluster)\n",
    "                \n",
    "                # Extract env and vault blocks from both files\n",
    "                common_env_block, common_vault_block = extract_values_info(common_values)\n",
    "                cluster_env_block, cluster_vault_block = extract_values_info(cluster_values)\n",
    "                \n",
    "                # Merge env and vault blocks, with cluster-specific values taking precedence\n",
    "                merged_env_block = common_env_block.copy()\n",
    "                merge_dicts(merged_env_block, cluster_env_block)\n",
    "                \n",
    "                merged_vault_block = common_vault_block if common_vault_block else {}\n",
    "                if cluster_vault_block:\n",
    "                    merge_dicts(merged_vault_block, cluster_vault_block)\n",
    "                \n",
    "                env_cluster_values[f'{environment}_{cluster}'] = {\n",
    "                    'vault': merged_vault_block,\n",
    "                    'env': merged_env_block\n",
    "                }\n",
    "\n",
    "    return env_cluster_values\n",
    "\n",
    "@stored\n",
    "def get_vault_value(path: str):\n",
    "    return vault_client.read(path=path)\n",
    "\n",
    "def fetch_vault_secrets(vault_data):\n",
    "    secrets = {}\n",
    "    if vault_data and vault_data['enabled']:\n",
    "        for vault_entry in vault_data['env']:\n",
    "            for path, keys in vault_entry.items():\n",
    "                vault_response = get_vault_value(path=path)\n",
    "                # print(path)\n",
    "                if vault_response:\n",
    "                    vault_secrets = vault_response['data']\n",
    "                    for key, alias in keys.items():\n",
    "                        if alias in vault_secrets:\n",
    "                            secrets[key] = vault_secrets[alias]\n",
    "    return secrets\n",
    "\n",
    "def extract_project_and_namespace(url):\n",
    "    match = re.match(r'https?://([^\\.]+)\\.([^\\.]+)', url)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2)\n",
    "    return None, None\n",
    "\n",
    "def create_neo4j_graph(configs):\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "    with driver.session(database=neo4j_db) as session:\n",
    "        for project, values in configs.items():\n",
    "            environment, cluster = project.split('_')\n",
    "            vault = values['vault']\n",
    "            vault_secrets = fetch_vault_secrets(vault)\n",
    "            if vault and vault['enabled']:\n",
    "                for secret_key, secret_value in vault_secrets.items():\n",
    "                    if isinstance(secret_value, str) and re.match(r'http[s]?://', secret_value):\n",
    "                        # print('pp')\n",
    "                        ref_project, namespace = extract_project_and_namespace(secret_value)\n",
    "                        if ref_project and namespace:\n",
    "                            session.run(\"MERGE (v:Vault {key: $secret_key, value: $secret_value})\", \n",
    "                                        secret_key=secret_key, secret_value=secret_value)\n",
    "                            session.run(\"\"\"\n",
    "                                MATCH (p:Project {name: $project_name}), (v:Vault {key: $secret_key, value: $secret_value})\n",
    "                                MERGE (p)-[:USES_VAULT]->(v)\n",
    "                            \"\"\", project_name=project, secret_key=secret_key, secret_value=secret_value)\n",
    "                            session.run(\"MERGE (rp:Project {name: $ref_project})\", ref_project=ref_project)\n",
    "                            session.run(\"MERGE (n:Namespace {name: $namespace})\", namespace=namespace)\n",
    "                            session.run(\"\"\"\n",
    "                                MATCH (p:Project {name: $project_name}), (rp:Project {name: $ref_project})\n",
    "                                MERGE (p)-[:INTERACTS_WITH {namespace: $namespace}]->(rp)\n",
    "                            \"\"\", project_name=project, ref_project=ref_project, namespace=namespace)\n",
    "\n",
    "def process_project(project_dir, session):\n",
    "    deploy_dir = os.path.join(project_dir, 'deploy')\n",
    "    \n",
    "    if not os.path.isdir(deploy_dir):\n",
    "        return\n",
    "    \n",
    "    values = extract_values(deploy_dir)\n",
    "    create_neo4j_graph(values)\n",
    "\n",
    "def main(base_dir):\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "    \n",
    "    with driver.session(database=neo4j_db) as session:\n",
    "        for project in os.listdir(base_dir):\n",
    "            try:\n",
    "                project_dir = os.path.join(base_dir, project)\n",
    "                print(project_dir)\n",
    "                if os.path.isdir(project_dir):\n",
    "                    process_project(project_dir, session)\n",
    "            except Exception as exc:\n",
    "                print(f'Error .\\nStack trace:\\n{traceback.format_exc()}')\n",
    "                # print('Error')\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "# Replace 'path/to/projects' with the actual path to the directory containing your projects\n",
    "# main('path/to/projects')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8e0ee-372e-4eb4-9d4f-4eee52f6d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dicts = [\n",
    "    '/Users/valuamba/wb/alljobswb/alljobswb',\n",
    "    # '/Users/valuamba/wb/hr',\n",
    "    # '/Users/valuamba/wb/hr/hr'\n",
    "]\n",
    "\n",
    "for path in group_dicts:\n",
    "    main(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c16132-36bf-41bf-bfcc-f0f55a03b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "service, environment, cluster, env, value\n",
    "crm-api, prod, k8s.stage-xc, DB_URL, postgres://sdgdsfgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
