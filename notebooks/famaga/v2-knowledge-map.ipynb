{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02a7b92-e1d2-4d59-85ba-ac318db6ca67",
   "metadata": {},
   "source": [
    "# Knowledge Map Liquidity Sample\n",
    "\n",
    "This requires:\n",
    "\n",
    "- `camelot-py[base]`\n",
    "- `openai`\n",
    "- `pandas`\n",
    "\n",
    "And also [pdftotext](https://en.wikipedia.org/wiki/Pdftotext)\n",
    "\n",
    "## Part 1: \"Framework\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92e0618-3ece-48d2-bc93-61a0e1e8eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a few helper utilities\n",
    "\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "text_cache = Path('cache')\n",
    "\n",
    "def sha1(input_string):\n",
    "    \"\"\"Helper to hash input strings\"\"\"\n",
    "    try:\n",
    "\n",
    "        # Step 5: Create a new SHA-1 hash object\n",
    "        hash_object = hashlib.sha1()\n",
    "\n",
    "        # Step 6: Update the hash object with the bytes-like object\n",
    "        hash_object.update(input_string.encode('utf-8'))\n",
    "\n",
    "        # Step 7: Get the hexadecimal representation of the hash\n",
    "        return hash_object.hexdigest()\n",
    "    except Exception as e:\n",
    "        raise ValueError(input_string) from e\n",
    "        \n",
    "def pdftotext(source: str, target: Path):\n",
    "    \"\"\"Extract text from PDF. Requires pdftotext binary in the path\"\"\"\n",
    "    \n",
    "    command = [\"pdftotext\", \"-enc\",\"UTF-8\",source, str(target)]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        if Path(target).exists():\n",
    "            Path(target).unlink()\n",
    "        print(f\"{source}: {result.stdout} {result.stderr}\".strip())\n",
    "\n",
    "def extract_text(source):\n",
    "    \"\"\"Get text from PDF. Cache results to avoid recomputation\"\"\"\n",
    "    local = text_cache / sha1(source.name + \" > text\")\n",
    "\n",
    "    if local.exists():\n",
    "        return local\n",
    "    print(f\"extracting from {source}\")\n",
    "    pdftotext(source, local)\n",
    "    return local\n",
    "\n",
    "def get_page_text(file):\n",
    "    \"\"\"Split extracted text into multiple pages\"\"\"\n",
    "    splits = extract_text(file).read_text().split(\"\\f\")\n",
    "\n",
    "    pages = []\n",
    "    for i, s in enumerate(splits):\n",
    "        pages.append(((i + 1), s))\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c92840-b94d-4d7f-acee-00d2e94b2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import inspect\n",
    "def stored(func):\n",
    "    \"\"\"\n",
    "    implements nix-like durable memoisation of function results.\n",
    "\n",
    "    Lazy way to avoid recomputing expensive calls. Expects results to be JSON-serializable\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def CACHE(*args, **kwargs):\n",
    "        name = func.__name__\n",
    "        meta = {}\n",
    "\n",
    "        meta[\"name\"] = name\n",
    "        meta[\"func\"] = inspect.getsource(func)\n",
    "        meta[\"args\"] = args\n",
    "        meta[\"kwargs\"] = kwargs\n",
    "\n",
    "        js = json.dumps(meta)\n",
    "        sha = hashlib.sha1(js.encode('utf-8'))\n",
    "\n",
    "        digest = sha.hexdigest()\n",
    "\n",
    "        path = text_cache / f\"{digest}-{name}.json\"\n",
    "\n",
    "        if path.exists():\n",
    "            with path.open('r') as r:\n",
    "                cached = json.load(r)\n",
    "            return cached[\"result\"]\n",
    "        result = func(*args, **kwargs)\n",
    "        meta[\"result\"] = result\n",
    "        with path.open('w') as w:\n",
    "            json.dump(meta, w)\n",
    "        return result\n",
    "\n",
    "    return CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b33560c-333b-49f8-8d83-bd4cb15ed9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import camelot\n",
    "\n",
    "@stored\n",
    "def extract_csvs(pdf, page, method):\n",
    "    \"\"\"Extract tables from the specified PDF page as CSV\"\"\"\n",
    "    try:\n",
    "        sourced = camelot.read_pdf(str(pdf), flavor=method, pages=str(page))\n",
    "        results = []\n",
    "        for s in sourced:\n",
    "            tbl = s.df.to_csv(index=False)\n",
    "            results.append(tbl)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f'Error on {pdf}' + e)\n",
    "        return []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f044ed47-1a16-40c4-a294-94a6c05b7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "@stored\n",
    "def get_gpt(content, model=\"gpt-4-1106-preview\", temperature=0, max_tokens=1000):\n",
    "    \"\"\"\n",
    "    Cached call to GPT.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return completion.model_dump()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df4b21-3435-405c-a68a-10e3d91c05b3",
   "metadata": {},
   "source": [
    "# Part 2: Extractors\n",
    "\n",
    "We have two:\n",
    "\n",
    "1. Simple extractor that determines company name for the annual report\n",
    "2. More complex extractor that gets liquidity values from the tables in annual report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b88e64a-e511-4c23-8456-b44998af759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_name(txt):\n",
    "\n",
    "    prompt = \"\"\"\n",
    "You are CFO-GPT. You read annual reports and identify the name of the company in that report. \n",
    "\n",
    "Respond only with the name of the company or business entity. Respond with an empty string, if none is found\n",
    "\n",
    "```txt\n",
    "$TXT\n",
    "```\"\"\".strip() + \"\\n\"\n",
    "    \n",
    "    filled = prompt.replace(\"$TXT\", txt)\n",
    "    result = get_gpt(filled)['choices'][0]['message']['content']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8322d518-d305-4f13-8171-b3747e50b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hey, GPTExtractMasterBot. I want to extract from tables in annual reports information about the liquidity. \n",
    "\n",
    "# What are the names of the tables that will contain this value? Focus only on the most likely and common table names. Answer with a python function that checks if string contains a table that is likely to contain liquidity information\n",
    "\n",
    "\n",
    "liquidity_tables = [\n",
    "    'balance sheet',\n",
    "    'cash flow statement',\n",
    "    'cash flows'\n",
    "]\n",
    "\n",
    "keywords = [\n",
    "    'cash and cash equivalents',\n",
    "     'cash flow from operating activities',\n",
    "    'free cash flow',\n",
    "    'liquid assets'\n",
    "]\n",
    "\n",
    "def contains_liquidity_data(txt):\n",
    "    \"\"\"\n",
    "    Check if the provided string contains a table name that is likely to contain liquidity information.\n",
    "\n",
    "    Args:\n",
    "    table_name (str): The name of the table to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the table_name is likely to contain liquidity information, False otherwise.\n",
    "    \"\"\"\n",
    "    # Lowercase the table_name to make the search case-insensitive\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # Define a list of common liquidity-related table names\n",
    "\n",
    "    \n",
    "    # Check if the table_name contains any of the liquidity-related table names\n",
    "    table_found = any(s in txt for s in liquidity_tables)\n",
    "    cell_found = any(s in txt for s in keywords)\n",
    "\n",
    "    return table_found and cell_found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8fe015-e69d-4315-98ea-be9e2cf07179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fail_table(df) -> str:\n",
    "    \"\"\"Quick heuristic to check if extracted data frame looks valid\"\"\"\n",
    "    txt = df.to_csv()\n",
    "\n",
    "\n",
    "    if \"....\" in txt:\n",
    "        return \"looks like index\"\n",
    "\n",
    "    shape = df.shape\n",
    "\n",
    "    surface = shape[0] * shape[1]\n",
    "    if surface < 7:\n",
    "        return f\"Surface is only {surface}\"\n",
    "\n",
    "    ratio = len(txt) / surface\n",
    "\n",
    "    if ratio > 40:\n",
    "        return f\"Text to cell ratio is {ratio}\"\n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "558ba2c0-359a-4bcc-bc39-3a98f9f549a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_liquidity_per_year(tables):\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    You are CFO-GPT. You read annual reports, looking for the amount of avalable liquidity (only cash and similar equivalents) in the annual reports.\n",
    "    \n",
    "    Pay attention to the units of measure of the tables. Return values as decimal number with a currency at the end.\n",
    "    \n",
    "    Answer with a json dictionary: `\"year\": \"Value UnitOfMeasurement Currency\"`. If value for the year is missing, then skip it.\n",
    "    \n",
    "    Do not write any code! Start your answer with \"```json\".\n",
    "    \n",
    "    # Source data to extract from\n",
    "    \n",
    "    Important! Pay attention to the units of measurement in each text chunk (usually at the beginning or the end of the chunk). \n",
    "    \n",
    "    $CSV\n",
    "    \n",
    "    \"\"\".strip() + \"\\n\"\n",
    "    \n",
    "    \n",
    "    csv_joined = \"\\n\".join(tables)\n",
    "    \n",
    "    \n",
    "    filled = prompt.replace(\"$CSV\", csv_joined)\n",
    "    \n",
    "    result = get_gpt(filled)['choices'][0]['message']['content']\n",
    "\n",
    "    try:\n",
    "    \n",
    "        data = json.loads(result.replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "    except:\n",
    "        raise ValueError(result)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f628dcb5-bf95-4f12-bf1b-381408029b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\MGroup\\components_agent_sales\\notebooks\\famaga\\tiny-data\\Bellevue Group AG - Annual_Report_2022.pdf\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m tables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 14\u001b[0m raw_pages \u001b[38;5;241m=\u001b[39m \u001b[43mget_page_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMGroup\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcomponents_agent_sales\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mnotebooks\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mfamaga\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtiny-data\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mBellevue Group AG - Annual_Report_2022.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m company_name \u001b[38;5;241m=\u001b[39m get_company_name(raw_pages[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m raw_pages[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Company name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m, in \u001b[0;36mget_page_text\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_page_text\u001b[39m(file):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split extracted text into multiple pages\"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     splits \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread_text()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m     pages \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splits):\n",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m, in \u001b[0;36mextract_text\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text\u001b[39m(source):\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get text from PDF. Cache results to avoid recomputation\"\"\"\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     local \u001b[38;5;241m=\u001b[39m text_cache \u001b[38;5;241m/\u001b[39m sha1(\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m > text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m local\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "sources = Path(r\"C:\\Users\\MGroup\\components_agent_sales\\notebooks\\famaga\\tiny-data\").rglob(\"*.pdf\")\n",
    "\n",
    "# print(list(sources))\n",
    "\n",
    "knowledge_map = []\n",
    "\n",
    "for pdf in sources:\n",
    "    print(f\"Reading {pdf}\")\n",
    "    tables = []\n",
    "\n",
    "    raw_pages = get_page_text(pdf)\n",
    "\n",
    "    company_name = get_company_name(raw_pages[0][1] + \"\\n\" + raw_pages[1][1])\n",
    "    \n",
    "    print(f\"  Company name: {company_name}\")\n",
    "\n",
    "    print(f\"  Extracting liquidity\")    \n",
    "    pages = []    \n",
    "    for num, txt in raw_pages:\n",
    "        if contains_liquidity_data(txt):\n",
    "            pages.append((num, txt))\n",
    "\n",
    "    print(f\"    interesting pages: {', '.join([str(t[0]) for t in pages])}\")\n",
    "    \n",
    "            \n",
    "    \n",
    "    final = []\n",
    "    for page, txt in pages:\n",
    "        for method in ['stream', 'lattice']:\n",
    "            found = False\n",
    "\n",
    "            csvs = extract_csvs(str(pdf), page, method)\n",
    "\n",
    "            for csv in csvs:\n",
    "                df = pd.read_csv(StringIO(csv))\n",
    "                fail_reason = fail_table(df)\n",
    "                if fail_reason:\n",
    "                    print(fail_reason)\n",
    "                    continue\n",
    "                final.append(f\"\\n```csv\\n{csv}\\n```\\n\\n\")\n",
    "    print(f\"    found {len(final)} tables\")\n",
    "          \n",
    "\n",
    "    if not final:\n",
    "        print(f\"Nothing found for {pdf}\")\n",
    "        continue\n",
    "    liquidity = extract_liquidity_per_year(final)\n",
    "\n",
    "    \n",
    "\n",
    "    record = {\"company\": company_name, \"liquidity\": liquidity}\n",
    "    print(\"Company map: \" + json.dumps(record, indent=2, ensure_ascii=False))\n",
    "\n",
    "    knowledge_map.append(record)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ec6a0b-aa88-45ac-ab2f-942c32c833a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'company': 'Bellevue Group',\n",
       "  'liquidity': {'2022': '64681 CHF', '2021': '84363 CHF'}},\n",
       " {'company': 'UNIQA Insurance Group AG',\n",
       "  'liquidity': {'2022': '667.675 Thousand €', '2021': '592.583 Thousand €'}},\n",
       " {'company': 'Christian Dior',\n",
       "  'liquidity': {'2022': '7.588 Billion EUR',\n",
       "   '2021': '8.122 Billion EUR',\n",
       "   '2020': '20.358 Billion EUR'}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the final knowledge map\n",
    "knowledge_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "911eb55a-b073-4d32-b2cd-29e0243b0311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine which company has more liquidity, we need to compare the liquidity figures for the most recent year provided, which is 2022. However, we need to ensure that we are comparing the values in the same currency. The liquidity for Bellevue Group is given in CHF (Swiss Francs), for UNIQA Insurance Group AG in thousands of EUR (Euros), and for Christian Dior in billions of EUR.\n",
      "\n",
      "First, we need to convert the liquidity of Bellevue Group and UNIQA Insurance Group AG to the same scale as Christian Dior, which is in billions of EUR. However, without the current exchange rates, we cannot make an accurate conversion. Since I cannot make up information, I will compare the numbers as they are, but please note that this comparison will not reflect the true liquidity due to the difference in currency values and scales.\n",
      "\n",
      "- Bellevue Group: 64,681 CHF\n",
      "- UNIQA Insurance Group AG: 667.675 thousand EUR (which is 0.667675 billion EUR)\n",
      "- Christian Dior: 7.588 billion EUR\n",
      "\n",
      "Based on the numbers provided, without converting currencies, Christian Dior has the most liquidity in 2022 with 7.588 billion EUR. UNIQA Insurance Group AG has 0.667675 billion EUR, and Bellevue Group has 64,681 CHF, which is significantly less than the figures in billions of EUR. However, to make an accurate comparison, you would need to convert all amounts to the same currency using the appropriate exchange rates.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are CFO-GPT. Quickly answer, which of the companies has more liquidity, and how much? Don't make up information, if you are not certain.\n",
    "\n",
    "```json\n",
    "$MAP\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "filled = prompt.replace(\"$MAP\", json.dumps(knowledge_map))\n",
    "result = get_gpt(filled)['choices'][0]['message']['content']\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
